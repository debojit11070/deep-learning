{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "6hrq52t8orV3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TXkVYDAd_IN2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "metadata": {
        "id": "qeURgWhR_Wz2"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download the Shakespeare dataset"
      ],
      "metadata": {
        "id": "VYBk9EbF_acM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt' )"
      ],
      "metadata": {
        "id": "kwozrnWB_YRk"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read the file"
      ],
      "metadata": {
        "id": "sZf71_0u_nMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read, then decode for py2 compat\n",
        "text = open(path, 'rb').read().decode(encoding='utf-8')"
      ],
      "metadata": {
        "id": "3YzRIhYQ_lMQ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#length of the text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxNKHoHc_0A3",
        "outputId": "7ce51796-ec0c-4b85-d626-0865ad7635cc"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# examin the first 250 characters\n",
        "print(text[:250])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6Ocv3YW_8nv",
        "outputId": "8540205e-3cbd-46b9-da34-f16893d25409"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dlml3rh9ADXm",
        "outputId": "8ba35307-edbd-4ad8-d19e-d22cfddaf4a9"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Process the text"
      ],
      "metadata": {
        "id": "WjJPwRTTAVhE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Vectrize the text </h3>"
      ],
      "metadata": {
        "id": "zKM4WNeKAZGO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before training, we need to convert hte string to a numerical representation"
      ],
      "metadata": {
        "id": "Z5g3M_fHC75K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88Ay493CAUZ3",
        "outputId": "d512c4f0-ffc9-4c9c-c66a-6c86f6953a3f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#now create the tf.keras.layers.stringLookup\n",
        "\n",
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary = list(vocab), mask_token=None\n",
        ")"
      ],
      "metadata": {
        "id": "bLmj1ZSgznCw"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hyx_SpFrz4Jb",
        "outputId": "d01dadfa-0d1e-40b1-b659-83188005c414"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the goal of this tutorial is to generate text, it will also be important to invert this representation and recover human-readable strings from it. For this you can use tf.keras.layers.StringLookup(..., invert=True)."
      ],
      "metadata": {
        "id": "w2wLkslj0DxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ],
      "metadata": {
        "id": "khG3-Wkvz7g0"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This layer recovers the characters from the vectors of IDs, and returns them as a tf.RaggedTensor of character\n",
        "chars = chars_from_ids(ids)\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpA_uhVq0RuZ",
        "outputId": "80bb5fae-5b20-4d5d-e511-f6d3fd7202d9"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we can tf.strings.reduce_join to join the characters back into strings.\n",
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DO7L6eIJ0ahH",
        "outputId": "b6e213c5-a3f0-4990-81af-2867ff0e7714"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis = -1)"
      ],
      "metadata": {
        "id": "EF_kylis0jlP"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The prediction task\n",
        "\n",
        "Given a character, or a sequence of characters, what is the most probable next character? This is the task you're training the model to perform. The input to the model will be a sequence of characters, and you train the model to predict the outputâ€”the following character at each time step.\n",
        "\n",
        "Since RNNs maintain an internal state that depends on the previously seen elements, given all the characters computed until this moment, what is the next character?\n",
        "\n",
        "# Create training examples and targets\n",
        "\n",
        "Next divide the text into example sequences. Each input sequence will contain seq_length characters from the text.\n",
        "\n",
        "For each input sequence, the corresponding targets contain the same length of text, except shifted one character to the right.\n",
        "\n",
        "So break the text into chunks of seq_length+1. For example, say seq_length is 4 and our text is \"Hello\". The input sequence would be \"Hell\", and the target sequence \"ello\".\n",
        "\n",
        "To do this first use the tf.data.Dataset.from_tensor_slices function to convert the text vector into a stream of character indices."
      ],
      "metadata": {
        "id": "Nyv7rxuQ02wd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UdBbyWF01Tj",
        "outputId": "eccdf872-aad4-4bb5-8cd5-2df1c7730b43"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "metadata": {
        "id": "MWH7EZQl1RBc"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "  print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmQC2FV71VOa",
        "outputId": "2450f684-e554-4537-ac00-e8a1a8ca9f19"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100\n"
      ],
      "metadata": {
        "id": "Gt-4g3ZO1fGZ"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder = True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b05B0yg31iMl",
        "outputId": "9e1becb0-4fc7-4b7b-a43a-679c29c1bfe1"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQRc2A2c1t52",
        "outputId": "a608d127-04a4-41c3-c545-43cc7101b4d9"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For training you'll need a dataset of (input, label) pairs. Where input and label are sequences. At each time step the input is the current character and the label is the next character.\n",
        "\n",
        "Here's a function that takes a sequence as input, duplicates, and shifts it to align the input and label for each timestep:"
      ],
      "metadata": {
        "id": "pG1e1mgG19Z1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(sequence):\n",
        "  input_text = sequence[:-1]\n",
        "  target_text = sequence[1:]\n",
        "  return input_text, target_text"
      ],
      "metadata": {
        "id": "BmcT7Cvm13_5"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqoTnJRJ2LrF",
        "outputId": "299cf417-da44-41ff-d2eb-0aec7aa7c554"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "F0iZf1AF2NLC"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZHx4kwA2PHT",
        "outputId": "661e153c-169b-49d3-dcfe-f32732f78ab8"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_MapDataset element_spec=(TensorSpec(shape=(100,), dtype=tf.int64, name=None), TensorSpec(shape=(100,), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdj5Uo6q2Pmp",
        "outputId": "ab4e6f1d-3c48-42f8-b52a-d40f33d8eb13"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create training batches\n",
        "we used tf.data to split the text into manageable sequences. But before feeding this data into the model, you need to shuffle the data and pack it into batches.\n"
      ],
      "metadata": {
        "id": "DOgmR2UL2UcK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000 # buffer size to shuffle the dataset\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
        ")"
      ],
      "metadata": {
        "id": "WUDHVp7Z2RwI"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build the model\n",
        "This section defines the model as a keras.Model subclass (For details see Making new Layers and Models via subclassing).\n",
        "\n",
        "This model has three layers:\n",
        "\n",
        "tf.keras.layers.Embedding: The input layer. A trainable lookup table that will map each character-ID to a vector with embedding_dim dimensions;\n",
        "tf.keras.layers.GRU: A type of RNN with size units=rnn_units (You can also use an LSTM layer here.)\n",
        "tf.keras.layers.Dense: The output layer, with vocab_size outputs. It outputs one logit for each character in the vocabulary. These are the log-likelihood of each character according to the model."
      ],
      "metadata": {
        "id": "W0RyzoS8E1V5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "vYDr7bPo2tLh"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "DFy3gEI1H9EF"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "PPrryEQRITw7"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkA5upJIJ7W7"
      },
      "source": [
        "For each character the model looks up the embedding, runs the GRU one timestep with the embedding as input, and applies the dense layer to generate logits predicting the log-likelihood of the next character:\n",
        "\n",
        "![A drawing of the data passing through the model](images/text_generation_training.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try the model\n"
      ],
      "metadata": {
        "id": "AsPUCB6nJGbn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIObbMY9IVYE",
        "outputId": "fefbb8ff-5302-4936-e8cb-17819d5cf3cb"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfYFS3uLIcEk",
        "outputId": "73960f72-acc1-4a22-9b29-a4d3eabaa1aa"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,022,850\n",
            "Trainable params: 4,022,850\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# try it for the first example in the batch\n",
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
        "sampled_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqZXDm8iJLdN",
        "outputId": "50feee88-4334-43a1-c787-fe53230f5f14"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([36, 31, 53, 35, 49, 40, 28, 30, 26,  1, 62, 59,  7, 49,  7, 40, 42,\n",
              "       65, 56, 17, 27, 62,  4,  8,  2,  5,  2, 26,  3,  9, 38, 12, 35, 29,\n",
              "       45, 35, 17, 37, 33, 37,  3, 36, 54,  0, 41, 20, 61, 22, 60, 54, 63,\n",
              "       31,  3, 26, 17, 20, 60, 49, 54,  4, 40, 54,  2, 21, 64, 31, 11, 55,\n",
              "       25, 10, 24, 13, 17, 25, 47, 41, 27, 10, 22, 59, 54, 53, 11, 39, 41,\n",
              "        1, 39, 30, 42,  4, 19, 35, 29, 57, 26, 51, 13, 27, 13, 33])"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0QoCMMQJS4M",
        "outputId": "754aa598-5460-4c41-dde0-a3249652749b"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b'RICHARD:\\nClifford, ask mercy and obtain no grace.\\n\\nEDWARD:\\nClifford, repent in bootless penitence.\\n\\n'\n",
            "\n",
            "Next Char Predictions:\n",
            " b'WRnVjaOQM\\nwt,j,aczqDNw$- & M!.Y;VPfVDXTX!Wo[UNK]bGvIuoxR!MDGujo$ao HyR:pL3K?DLhbN3Iton:Zb\\nZQc$FVPrMl?N?T'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the model"
      ],
      "metadata": {
        "id": "SabcR09PJbAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "_PEH9yRhJZ0y"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORYLtdRnJd3V",
        "outputId": "31fc749d-4bd7-4ed8-f51a-f1b9465fdd21"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.1879344, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIH7RuWHJexm",
        "outputId": "5d3c67d6-6302-454f-865b-0f72055bf396"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65.88655"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "PuHC9ibiJgzH"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "NGZ3BZXiJhue"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# execute the training\n",
        "EPOCHS = 20\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xD6Hd-aZJjoz",
        "outputId": "29394ee5-6ae8-4169-dba7-4aef769c203c"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "172/172 [==============================] - 16s 57ms/step - loss: 2.6921\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 12s 56ms/step - loss: 1.9735\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 11s 53ms/step - loss: 1.6934\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 11s 52ms/step - loss: 1.5344\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 11s 52ms/step - loss: 1.4383\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 11s 52ms/step - loss: 1.3712\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 11s 52ms/step - loss: 1.3184\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 1.2733\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 11s 53ms/step - loss: 1.2322\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 12s 53ms/step - loss: 1.1917\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 11s 52ms/step - loss: 1.1498\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 11s 53ms/step - loss: 1.1071\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 11s 53ms/step - loss: 1.0613\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 11s 53ms/step - loss: 1.0138\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 12s 53ms/step - loss: 0.9641\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - 12s 54ms/step - loss: 0.9118\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - 11s 54ms/step - loss: 0.8589\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - 11s 54ms/step - loss: 0.8060\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 0.7550\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - 12s 54ms/step - loss: 0.7078\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text\n",
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "metadata": {
        "id": "GSUMK-tDJtxC"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "metadata": {
        "id": "E_TPG2iMJx5m"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVuH8854J0Qa",
        "outputId": "74bdc51d-b75b-4c87-f4f2-22019c536034"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "Ah, my church ange; no, and on my knee must blame.\n",
            "See that my daughter had betray my care return.\n",
            "\n",
            "KING RICHARD III:\n",
            "Sirrah young Venow, sworn begins to make\n",
            "His body likeness to a most gaod.\n",
            "\n",
            "MOMNA:\n",
            "I have got him that walls, and durst not nurse?\n",
            "\n",
            "Nark:\n",
            "My Lord Fitzis a good suitor,\n",
            "Even thought upon him.\n",
            "\n",
            "All Those that are you!\n",
            "Methinks that hand on seem his birthres like cords spriges;\n",
            "My dagger is well slay-mings, all\n",
            "his lords,\n",
            "Shall fill'd in Henry lives, they come not,\n",
            "Lood Katharina with them above,\n",
            "And change our neck of his great grief,\n",
            "With all his lordship trunked long,\n",
            "Nor citizels not to London England's royal king;\n",
            "Lest blowing betwixt his blood and sweet silver\n",
            "Coring up: he seat of friends!\n",
            "But let your restlusings that the time\n",
            "'Your injuries are both beginness.\n",
            "\n",
            "SOMERSET:\n",
            "Say, Sbaching for your loves, untie me study\n",
            "In whom a part of what it bodes; and\n",
            "therefore I'll play my pash all I know;\n",
            "And much I betray his very nor herself,\n",
            "For I toward the nobles shows wha \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.5607621669769287\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CnCFUdycJ1W1",
        "outputId": "642fc77e-5c68-4476-db62-935ca626f874"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nI spy king and damnabling hoped!\\nOf much I am too out. If it be sound,\\nOr hollow princess; but that you own to speak upon his\\nmealous. Hark you, he loves a bear.\\nDarry, and some recreant and married; mock degree?\\nNo, but mistaking come again,--\\nNurse, come; for I know,\\nSince I turn my house these may execute,\\nOr sleeples, the secrets it's ranishment,\\nThat it may jest on me, and thy\\nsupper-each our gates, nightly to his country:\\nHe humbles there did neelf villains, that\\nCame but emboation; on thy eye I pluck my language.\\n\\nHENRY BOLINGBROKE:\\nYour will take ord and will not do me but speak some: the people\\nDe'er through'd you at the outward kept?\\n\\nGET:\\nHere comes the cause. why, lo, her nothing, all;\\nbut I have ta'en thee perform'd, but cannot'st be\\npalsed here, till he comes.\\n\\nFirst Senator:\\nTrue; how is't! spoke have the king is detched?\\nThou hast done so. Alack, an heir,\\nBut fright out mercy, where art thou the day,\\nSo foul the duke a flight, throw awake stones\\nwith the remorse; but, \"\n",
            " b\"ROMEO:\\nNow, afore God, 'tis shame put upon 'im.\\n\\nClown:\\nHis bloody hand that was my countenanced.\\nThis hast thou tander, urge itself your spirits:\\nCome, cheery, as he serving vercy's banish'd:\\nBut in excese condition stooping at my feet,\\nFor I have heard some cannot call him seem to weep.\\n\\nFRIAR PETER:\\nI know 'tis so: but to return we thought,\\nMuch more there but my grieved; yet,\\nWert thou but stay?\\n\\nESCALUS:\\nI am content you shall here proclamation: yea,\\nKatharina, the downfall of our drinent,\\nShalt reputation will her in one little off,\\nThat cries 'CApilous hurt in heaven and began to live.\\nShall I be youngest, I will not know.\\n\\nPedant:\\nHe is too general;\\nNo first begut you out, and it confessor,\\nGrace to pursue the crown, and thy aught it gid\\nyou straight,\\nVietest his patrifice, and that it was my wife\\nThat every man here for better proudly doath.\\nThere is, that cannot be abuted with tee rear.\\nI protest, he waits upon your grace.\\n\\nKING RICHARD III:\\nHe firty sir, the bride rebouges,\\nAnd wh\"\n",
            " b\"ROMEO:\\nThe wrongs on Johem, at holy liver\\nHath patiented lower or an unglain'd friends,\\nNay, would a lovely guilt I give;\\nAnd for become them for his brother and his lands are grace.\\nInstruct us instantly,--for his obedience?\\nO love! O life! love! now; not any grief.\\nYour brother's son, that would a longly hand,\\nAnd in Absence doth live; and stand upon\\n'T: how art thou remounded, the tenthy and the\\nForfeit of them; you know;\\nHere stands the corn, our will, and brave Mantua,\\nTo the streets of men mistake.\\n\\nLord:\\nHow! the mayor of any offer over-certain recompe\\nForbid and not our former deeds doublet begin?\\n\\nLORD What says he?\\n\\nCATESBY:\\nHere, my delight and Montague,\\nYou may carry it!\\nShe is lording of my kind mine arty.\\n\\nPETRUCHIO:\\nA holy!\\n\\nCitizens:\\nYou shall have this offence out.\\n\\nCAPULET:\\nYou? why, my master what with it back infect anon. Away!\\nWell, bear you for our designing at thy attorneys;\\nSole money by me Disgor in my sword,\\nNor father. The poor hollow-painted flower\\nProvoked 'banis\"\n",
            " b\"ROMEO:\\nWhat means our cousin's chaiting here?\\n\\nQUEEN:\\nHis noble seeming, stand by up,\\nThan done the gates of Sistoo has so sour\\nWho parting.\\n\\nMERCUTIO:\\nThat plainly have well provers best descend;\\nAnd cat not now in a daughter of inquarrel thirst\\nWhich seems unwashel'd but as after-trieved\\nof mercy, and his looks before him.\\n\\nANGELO:\\nShe is in readiest waver he endure his memany\\nTo entur. He is for policion; how\\nAgainst all tailor with the crown?\\n\\nJULIET:\\nShe is not ambusy for Henry's friend.\\n\\nWARWICK:\\nWhy art thou so? the kespectal of our business\\nI am touch'd with promorty; young\\nHave thrust down can may drink the rebold of him\\nis hath thus past befleme so bright.\\n\\nPROSPERO:\\nCome, fellow dropprey, leave, we find that oath;\\nAnd lay as he, upon a woman's chamnish;\\nTunner himself must hither with their prodides,\\nThat feed'st me chidegain after things affairs;\\nThe flatterer for the morning's face,\\nThat fear'd I have give us not, I now object\\nI shall approve me.' And yet, I warrant ye.\\n\\nPETRUCH\"\n",
            " b\"ROMEO:\\nThat, when we have a coisardly rage, like spleens;\\nOr else I swear to the runch.\\n\\nBRUTUS:\\nIt will rewel it his new to tell me I complaint\\nIn that word 'ELIZABETH:\\nO ho, a royal ancient skills?\\nHappier the sword or perpetual home,\\nAnd honesty to sight! you fight again,\\nIt by the attempt of his comfortable wrinkles,\\nNorning for his coward! ay, but this\\nThat way betides.\\n\\nFiestephio, gentle nirech; can he died:\\nNay, now no more: my lord, we have any's dam.\\nHave long sushician and my provincip in\\nYour special prayer or emmoting.\\n\\nPETRUCHIO:\\nVillain, I say so ill.\\nI will unto Juliet, fortune stake three palace;\\nWhilst thou love me, didst thou never stand\\nme still an oath while I at heaven with age.\\nSweet lady, pleased, and better part.\\n\\nCAPULET:\\nYoung Romeo is earth?\\n'Tis bother, and I not advise my father;\\nThen is the river and clook unto me solut:\\nI do well forbearning him to sound to heal the sword;\\nOf the whole sattling father stands the disposition\\nof water-fortunes and my new burning\"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.2125189304351807\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cjdAWqIZJ29e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}